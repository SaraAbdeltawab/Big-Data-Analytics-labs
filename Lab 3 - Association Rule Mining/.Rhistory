fit5 <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3, minbucket=4),
parms=list(split='information'))
# split='information' : means split on "information gain"
#plot the tree
rpart.plot(fit5, type = 4, extra = 1)
#graph2
fit5 <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
method="class",
data=play_decision,
control=rpart.control(minsplit=2, maxdepth = 3, minbucket=3),
parms=list(split='information'))
# split='information' : means split on "information gain"
#plot the tree
rpart.plot(fit5, type = 4, extra = 1)
#############################################################
#Q3: What will happen if only one of either minsplit or minbucket is specified
#    and not the other?
#If only one of minbucket or minsplit is specified, the code either sets minsplit to minbucket*3
#or minbucket to minsplit/3, as appropriate.
#Q4: What does 'type' and 'extra' parameters mean in the plot function?
if(FALSE){
"
type: different representation of the tree
extra: Display extra information at the nodes.
check different examples below.
From Documentation:
Type Possible values:
0: Draw a split label at each split and a node label at each leaf.
1 Label all nodes, not just leaves.
2 Default. Like 1 but draw the split labels below the node labels.
3 Draw separate split labels for the left and right directions.
4 Like 3 but label all nodes, not just leaves.
extra Possible values:
auto (case insensitive) Default.
Automatically select a value based on the model type, as follows:
extra=106 class model with a binary response
extra=104 class model with a response having more than two levels
extra=100 other models
0 No extra information.
1 Display the number of observations that fall in the node (per class for class objects; prefixed by the number of events for poisson and exp models)
2 Class models: display the classification rate at the node, expressed as the number of correct classifications and the number of observations in the node.
Poisson and exp models: display the number of events.
3 Class models: misclassification rate at the node, expressed as the number of incorrect classifications and the number of observations in the node.
4 Class models: probability per class of observations in the node (conditioned on the node, sum across a node is 1).
5 Class models: like 4 but don't display the fitted class.
6 Class models: the probability of the second class only. Useful for binary responses.
7 Class models: like 6 but don't display the fitted class.
8 Class models: the probability of the fitted class.
9 Class models: The probability relative to all observations - the sum of these probabilities across all leaves is 1. This is in contrast to the options above, which give the probability relative to observations falling in the node - the sum of the probabilities across the node is 1.
"
}
rpart.plot(fit, type = 0, extra = 1)
rpart.plot(fit, type = 0, extra = 0)
rpart.plot(fit, type = 2, extra = 3)
#Q5: Plot the tree with propabilities instead of number of observations in each node.
######################################################################################
rpart.plot(fit, type = 4, extra = 9)
#Predict if Play is possible for condition rainy, mild humidity, high temperature and no wind
newdata <- data.frame(Outlook="overcast",Temperature="mild",Humidity="high",Wind=FALSE)
newdata
predict(fit,newdata=newdata,type=c("class"))
# type can be class, prob or vector for classification trees.
######################################################################################
#Q6: What is the predicted class for this test case?
#yes => play
#Q7: State the sequence of tree node checks to reach this class (label).
#1- temperature mild => go left
#2- outlook  overcast => go right
"taking the last represnted graph as reference"
## ================================= END ===================================== ##
# Let us get the appropriate libraries loaded for NB Classifier.
#install.packages("e1071")
library("e1071")
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 4 - Classification\\Lab")
###########################
#req2
dfm <- read.csv("nbtrain.csv")
dim(dfm)
#What are the variables of this data set?
#age, gender, educ, income "in a categorial form"
##########################
#req3
traindata <- dfm[1:9000,]
testdata <- dfm[9001:nrow(dfm),]
#Display data frames
traindata
testdata
#Why do we split data into training and test sets?
#trining set is the initial dataset used to train the algorithm
#test set to test the performance of our model and adjust the hyper parameters
#and if test if there is an overfitting/underfitting issues
#we can’t  reuse the training dataset in testing
#because the algorithm will already know the expected output
#########################
#req4
# use the NB classifier with Laplace smoothing
model <- naiveBayes(income ~.,traindata,laplace=.01)
########################
#req5
#display model
model
#########################
#req6
# predict with testdata
results <- predict (model,testdata)
# display results
results
#What does Laplace smoothing coefficient mean?
#positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing.
#to solve the problem of zero probability.
#In NaiveBayes: we are multiplying all the probabilities during inference
#One zero probability term will lead to the entire process failing.
#the goal is to increase the zero probability values to a small positive number
#and  reduce other values so that the sum is still 1
#Laplace smoothing is one such method
##########################
#req7
#confusionMatrix(data, reference, positive = NULL, dnn = c("Prediction", "Reference"), ...)
table(result, testdata$income)
# Let us get the appropriate libraries loaded for NB Classifier.
#install.packages("e1071")
library("e1071")
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 4 - Classification\\Lab")
###########################
#req2
dfm <- read.csv("nbtrain.csv")
dim(dfm)
#What are the variables of this data set?
#age, gender, educ, income "in a categorial form"
##########################
#req3
traindata <- dfm[1:9000,]
testdata <- dfm[9001:nrow(dfm),]
#Display data frames
traindata
testdata
#Why do we split data into training and test sets?
#trining set is the initial dataset used to train the algorithm
#test set to test the performance of our model and adjust the hyper parameters
#and if test if there is an overfitting/underfitting issues
#we can’t  reuse the training dataset in testing
#because the algorithm will already know the expected output
#########################
#req4
# use the NB classifier with Laplace smoothing
model <- naiveBayes(income ~.,traindata,laplace=.01)
########################
#req5
#display model
model
#########################
#req6
# predict with testdata
results <- predict (model,testdata)
# display results
results
#What does Laplace smoothing coefficient mean?
#positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing.
#to solve the problem of zero probability.
#In NaiveBayes: we are multiplying all the probabilities during inference
#One zero probability term will lead to the entire process failing.
#the goal is to increase the zero probability values to a small positive number
#and  reduce other values so that the sum is still 1
#Laplace smoothing is one such method
##########################
#req7
#confusionMatrix(data, reference, positive = NULL, dnn = c("Prediction", "Reference"), ...)
table(results, testdata$income)
?accuracy
# Let us get the appropriate libraries loaded for NB Classifier.
#install.packages("e1071")
library("e1071")
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 4 - Classification\\Lab")
###########################
#req2
dfm <- read.csv("nbtrain.csv")
dim(dfm)
#What are the variables of this data set?
#age, gender, educ, income "in a categorial form"
##########################
#req3
traindata <- dfm[1:9000,]
testdata <- dfm[9001:nrow(dfm),]
#Display data frames
traindata
testdata
#Why do we split data into training and test sets?
#trining set is the initial dataset used to train the algorithm
#test set to test the performance of our model and adjust the hyper parameters
#and if test if there is an overfitting/underfitting issues
#we can’t  reuse the training dataset in testing
#because the algorithm will already know the expected output
#########################
#req4
# use the NB classifier with Laplace smoothing
model <- naiveBayes(income ~.,traindata,laplace=.01)
########################
#req5
#display model
model
#########################
#req6
# predict with testdata
results <- predict (model,testdata)
# display results
results
#What does Laplace smoothing coefficient mean?
#positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing.
#to solve the problem of zero probability.
#In NaiveBayes: we are multiplying all the probabilities during inference
#One zero probability term will lead to the entire process failing.
#the goal is to increase the zero probability values to a small positive number
#and  reduce other values so that the sum is still 1
#Laplace smoothing is one such method
##########################
#req7
#confusionMatrix(data, reference, positive = NULL, dnn = c("Prediction", "Reference"), ...)
tb <- table(results, testdata$income)
#Investigate the results.
#Explain the variation in the model’s classification power across income classes.
#modify
############################
#req8
#accuracy = sum diagol/total
accuracy <- sum(diag(tb))/sum(tb)
accuracy
# Let us get the appropriate libraries loaded for NB Classifier.
#install.packages("e1071")
library("e1071")
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 4 - Classification\\Lab")
###########################
#req2
dfm <- read.csv("nbtrain.csv")
dim(dfm)
#What are the variables of this data set?
#age, gender, educ, income "in a categorial form"
##########################
#req3
traindata <- dfm[1:9000,]
testdata <- dfm[9001:nrow(dfm),]
#Display data frames
traindata
testdata
#Why do we split data into training and test sets?
#trining set is the initial dataset used to train the algorithm
#test set to test the performance of our model and adjust the hyper parameters
#and if test if there is an overfitting/underfitting issues
#we can’t  reuse the training dataset in testing
#because the algorithm will already know the expected output
#########################
#req4
# use the NB classifier with Laplace smoothing
model <- naiveBayes(income ~.,traindata,laplace=.01)
########################
#req5
#display model
model
#########################
#req6
# predict with testdata
results <- predict (model,testdata)
# display results
results
#What does Laplace smoothing coefficient mean?
#positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing.
#to solve the problem of zero probability.
#In NaiveBayes: we are multiplying all the probabilities during inference
#One zero probability term will lead to the entire process failing.
#the goal is to increase the zero probability values to a small positive number
#and  reduce other values so that the sum is still 1
#Laplace smoothing is one such method
##########################
#req7
#confusionMatrix(data, reference, positive = NULL, dnn = c("Prediction", "Reference"), ...)
tb <- table(results, testdata$income)
#Investigate the results.
#Explain the variation in the model’s classification power across income classes.
#modify
############################
#req8
#accuracy = sum diagol/total
accuracy <- sum(diag(tb))/sum(tb)
accuracy
#modify
###########################
missclassification <- (1-accuracy)
missclassification
tb <- table(results, testdata$income)
tb
#req8
rules_by_support <- sort(rules)
inspect(rules_by_support[1:6])
#req2
library(arules)
library(arulesViz)
#############################
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining")
#############################
#req3
transactions <- read.transactions("AssociationRules.csv", header=FALSE)
dim(transactions)
##Make sure you don’t include the header line in the dataset ??#modify
###############################
#req4
inspect(transactions[1:100,])
##############################
#req5
#itemFrequency(transactions)
summary(transactions)
##the most frequent two items in the dataset
##item13 = 4948   item5 = 3699
#############################
#req6
itemFrequencyPlot(transactions,topN = 5)
#############################
#req7
rules <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.5, minlen = 2))
rules
#############################
#req8
rules_by_support <- sort(rules)
inspect(rules_by_support[1:6])
inspect(rules_by_confidence[1:6])
rules_by_confidence <- sort(rules, by="conf")
inspect(rules_by_confidence[1:6])
rules_by_lift <- sort(rules, by="lift")
inspect(rules_by_lift[1:6])
plot(rules, measure = c("support", "confidence"), shading = "lift", jitter = 0)
tb <- table(results, testdata$income)
tb
# Let us get the appropriate libraries loaded for NB Classifier.
#install.packages("e1071")
library("e1071")
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 4 - Classification\\Lab")
###########################
#req2
dfm <- read.csv("nbtrain.csv")
dim(dfm)
#What are the variables of this data set?
#age, gender, educ, income "in a categorial form"
##########################
#req3
traindata <- dfm[1:9000,]
testdata <- dfm[9001:nrow(dfm),]
#Display data frames
traindata
testdata
#Why do we split data into training and test sets?
#trining set is the initial dataset used to train the algorithm
#test set to test the performance of our model and adjust the hyper parameters
#and if test if there is an overfitting/underfitting issues
#we can’t  reuse the training dataset in testing
#because the algorithm will already know the expected output
#########################
#req4
# use the NB classifier with Laplace smoothing
model <- naiveBayes(income ~.,traindata,laplace=.01)
########################
#req5
#display model
model
#########################
#req6
# predict with testdata
results <- predict (model,testdata)
# display results
results
#What does Laplace smoothing coefficient mean?
#positive double controlling Laplace smoothing. The default (0) disables Laplace smoothing.
#to solve the problem of zero probability.
#In NaiveBayes: we are multiplying all the probabilities during inference
#One zero probability term will lead to the entire process failing.
#the goal is to increase the zero probability values to a small positive number
#and  reduce other values so that the sum is still 1
#Laplace smoothing is one such method
##########################
#req7
#confusionMatrix(data, reference, positive = NULL, dnn = c("Prediction", "Reference"), ...)
tb <- table(results, testdata$income)
tb
#req2
library(arules)
library(arulesViz)
#############################
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining")
#############################
#req3
transactions <- read.transactions("AssociationRules.csv", header=FALSE)
dim(transactions)
##Make sure you don’t include the header line in the dataset ??#modify
###############################
#req4
inspect(transactions[1:100,])
##############################
#req5
#itemFrequency(transactions)
summary(transactions)
##the most frequent two items in the dataset
##item13 = 4948   item5 = 3699
#############################
#req6
itemFrequencyPlot(transactions,topN = 5)
#############################
#req7
rules <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.5, minlen = 2))
rules
#############################
#req8
rules_by_support <- sort(rules)
inspect(rules_by_support[1:6])
###########################
#req9
rules_by_confidence <- sort(rules, by="conf")
inspect(rules_by_confidence[1:6])
##########################
#req10
rules_by_lift <- sort(rules, by="lift")
inspect(rules_by_lift[1:6])
##########################
#req11
plot(rules, measure = c("support", "confidence"), shading = "lift", jitter = 0)
##########################
#req12
#Based on (8-11)
#Can you tell now what are the most interesting rules that are really useful and provide a real business value and an insight to the concerned corporate?
#with larger values of Lift we may say the rule is true and not coincidental.
#
#req2
library(arules)
library(arulesViz)
#############################
#req1
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 3 - Association Rule Mining\\Lab 3 - Association Rule Mining")
#############################
#req3
transactions <- read.transactions("AssociationRules.csv", header=FALSE)
dim(transactions)
##Make sure you don’t include the header line in the dataset ??#modify
###############################
#req4
inspect(transactions[1:100,])
##############################
#req5
#itemFrequency(transactions)
summary(transactions)
##the most frequent two items in the dataset
##item13 = 4948   item5 = 3699
#############################
#req6
itemFrequencyPlot(transactions,topN = 5)
#############################
#req7
rules <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.5, minlen = 2))
rules
#############################
#req8
rules_by_support <- sort(rules)
inspect(rules_by_support[1:6])
###########################
#req9
rules_by_confidence <- sort(rules, by="conf")
inspect(rules_by_confidence[1:6])
##########################
#req10
rules_by_lift <- sort(rules, by="lift")
inspect(rules_by_lift[1:6])
##########################
#req11
plot(rules, measure = c("support", "confidence"), shading = "lift", jitter = 0)
##########################
#req12
#Based on (8-11)
#Can you tell now what are the most interesting rules that are really useful and provide a real business value and an insight to the concerned corporate?
# values with high  Lift we may say the rule is true and not coincidental.
#So we take rules with high lift and confidence..
#such as
#[1] {item15,item30,item56} => {item49} 0.0101  0.7709924  19.42046 101
#[2] {item30,item56,item84} => {item49} 0.0100  0.7407407  18.65846 100
