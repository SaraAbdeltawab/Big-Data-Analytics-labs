install.packages(c("forecast", "TSPred"))
#In this tutorial we are going to examine a data series about monthly car sales
require(forecast)
require(TSPred)
#Import the time series data in 'tractor-sales.csv' file.
tsales <- read.table("Tractor-Sales.csv", header=TRUE, sep=",")
#(1) Inspect the data
tsales
data <- ts(tsales[,2], start = c(2003,1), frequency = 12)
?ts
par(mfrow=c(1,1))
plot(data, xlab="Years", ylab = "Tractor Sales")
#(4)
plot(stl(data, "periodic"))
?stl
plot(data, xlab="Years", ylab = "Tractor Sales")
#In this tutorial we are going to examine a data series about monthly car sales
require(forecast)
require(TSPred)
#Import the time series data in 'tractor-sales.csv' file.
tsales <- read.table("Tractor-Sales.csv", header=TRUE, sep=",")
#(1) Inspect the data
tsales
#(2) Create a time series object out of the sales data.
data <- ts(tsales[,2], start = c(2003,1), frequency = 12) #??modify
#(3) Visualise the time series.
par(mfrow=c(1,1))
plot(data, xlab="Years", ylab = "Tractor Sales")
#(4)
plot(stl(data, "periodic"))
#(5)
plot(data, xlab="Years", ylab = "Tractor Sales")
#(6)
plot(diff(data), ylab="Differenced Tractor Sales")
#In this tutorial we are going to examine a data series about monthly car sales
require(forecast)
require(TSPred)
#clean environment
rm(list=ls())
#Import the time series data in 'tractor-sales.csv' file.
tsales <- read.table("Tractor-Sales.csv", header=TRUE, sep=",")
#(1) Inspect the data
tsales
#(2) Create a time series object out of the sales data.
data <- ts(tsales[,2], start = c(2003,1), frequency = 12) #??modify
#(3) Visualise the time series.
par(mfrow=c(1,1))
plot(data, xlab="Years", ylab = "Tractor Sales")
#(4)
plot(stl(data, "periodic"))
#(5)
plot(data, xlab="Years", ylab = "Tractor Sales")
require(forecast)
require(TSPred)
#clean environment
rm(list=ls())
#Import the time series data in 'tractor-sales.csv' file.
tsales <- read.table("Tractor-Sales.csv", header=TRUE, sep=",")
#(1) Inspect the data
tsales
#(2) Create a time series object out of the sales data.
data <- ts(tsales[,2], start = c(2003,1), frequency = 12) #??modify
#(3) Visualise the time series.
par(mfrow=c(1,1))
plot(data, xlab="Years", ylab = "Tractor Sales")
#(4)
plot(stl(data, "periodic"))
#(5)
plot(data, xlab="Years", ylab = "Tractor Sales")
#(6)
plot(diff(data), ylab="Differenced Tractor Sales")
plot(log10(data),ylab="Log (car Sales)")
#(5)
plot(data, xlab="Years", ylab = "Tractor Sales")
#(7)
plot(log10(data),ylab="Log (car Sales)")
#(8)
plot(diff(log10(data)),ylab="Differenced Log (car Sales)")
plot(diff(data), ylab="Differenced Tractor Sales")
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=FALSE)
summary(ARIMAfit)
?auto.arima
#(9) Fitting an ARIMA Model
ARIMAfit <- auto.arima(diff(log10(data)), approximation=FALSE, trace=FALSE)
#auto.arima decide the best model
summary(ARIMAfit)
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=TRUE)
summary(ARIMAfit)
#In this tutorial we are going to examine a data series about monthly car sales
require(forecast)
require(TSPred)
#clean environment
rm(list=ls())
#Import the time series data in 'tractor-sales.csv' file.
tsales <- read.table("Tractor-Sales.csv", header=TRUE, sep=",")
tsales
#(2) Create a time series object out of the sales data.
data <- ts(tsales[,2], start = c(2003,1), frequency = 12)
par(mfrow=c(1,1))
plot(data, xlab="Years", ylab = "Tractor Sales")
plot(stl(data, "periodic"))
plot(data, xlab="Years", ylab = "Tractor Sales")
plot(diff(data), ylab="Differenced Tractor Sales")
plot(log10(data),ylab="Log (car Sales)")
plot(diff(data), ylab="Differenced Tractor Sales")
plot(log10(data),ylab="Log (car Sales)")
plot(diff(data), ylab="Differenced Tractor Sales")
plot(log10(data),ylab="Log (car Sales)")
plot(diff(log10(data)),ylab="Differenced Log (car Sales)")
plot(log10(data),ylab="Log (car Sales)")
plot(diff(log10(data)),ylab="Differenced Log (car Sales)")
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=FALSE)
plot(log10(data),ylab="Log (car Sales)")
summary(ARIMAfit)
ARIMAfit <- auto.arima(diff(log10(data)), approximation=FALSE, trace=FALSE)
# Try to pass diff(log10(data)) instead of log10(data) and see the difference
# in the output model.
#auto.arima decide the best model
summary(ARIMAfit)
ARIMAfit <- auto.arima(diff(log10(data)), approximation=FALSE, trace=TRUE)
# Try to pass diff(log10(data)) instead of log10(data) and see the difference
# in the output model.
#auto.arima decide the best model
summary(ARIMAfit)
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=FALSE)
# Try to pass diff(log10(data)) instead of log10(data) and see the difference
# in the output model.
#auto.arima decide the best model
summary(ARIMAfit)
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=TRUE)
# Try to pass diff(log10(data)) instead of log10(data) and see the difference
# in the output model.
#auto.arima decide the best model
summary(ARIMAfit)
?auto.arima
pred <- predict(ARIMAfit, n.ahead = 36)
?predict
plot((data),type="l",xlim=c(2004,2018), ylim=c(1,1600), xlab = "Year",ylab = "car")
lines(10^(pred$pred),col="blue")
lines(10^(pred$pred+2*pred$se),col="orange")
lines(10^(pred$pred-2*pred$se),col="orange")
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2018), range.percent = 0.2
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2022), range.percent = 0.2)
require(TSPred)
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2022), range.percent = 0.2)
#(13) library TSPred
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2018), range.percent = 0.2)
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2018), range.percent = 0.2)
lines(10^(pred$pred),col="blue")
lines(10^(pred$pred),col="blue")
plot((data),type="l",xlim=c(2004,2018), ylim=c(1,1600), xlab = "Year",ylab = "car")
lines(10^(pred$pred),col="blue")
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2018), range.percent = 0.2)
plotarimapred(data, ARIMAfit, xlim=c(2004,2018), range.percent = 0.2)
#In this tutorial we are going to examine a data series about monthly car sales
require(forecast)
require(TSPred)
#clean environment
rm(list=ls())
#Import the time series data in 'tractor-sales.csv' file.
tsales <- read.table("Tractor-Sales.csv", header=TRUE, sep=",")
#(1) Inspect the data
tsales
#(2) Create a time series object out of the sales data.
data <- ts(tsales[,2], start = c(2003,1), frequency = 12) #??modify
#(3) Visualise the time series.
par(mfrow=c(1,1))
plot(data, xlab="Years", ylab = "Tractor Sales")
#(4)
plot(stl(data, "periodic"))
#(5)
plot(data, xlab="Years", ylab = "Tractor Sales")
#(6)
plot(diff(data), ylab="Differenced Tractor Sales")
#(7)
plot(log10(data),ylab="Log (car Sales)")
#(8)
plot(diff(log10(data)),ylab="Differenced Log (car Sales)")
#(9) Fitting an ARIMA Model
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=TRUE)
# Try to pass diff(log10(data)) instead of log10(data) and see the difference
# in the output model.
#auto.arima decide the best model
summary(ARIMAfit)
#(10) Fitting an ARIMA Model
ARIMAfit <- auto.arima(log10(data), approximation=FALSE, trace=TRUE)
summary(ARIMAfit)
#(11) Use the ARIMA model to forecast values.
pred <- predict(ARIMAfit, n.ahead = 36)
#(12) Plot the original data
plot((data),type="l",xlim=c(2004,2018), ylim=c(1,1600), xlab = "Year",ylab = "car")
#plot the prediction and its +- 2 std dev. (expected error)
lines(10^(pred$pred),col="blue")
lines(10^(pred$pred+2*pred$se),col="orange")
lines(10^(pred$pred-2*pred$se),col="orange")
#Note that this is the reason why we passed log10(data) to the auto.arima
#instead of diff(log10(data).
#If we passed diff(log10(data)), then in order to forecast the future values
#in the time series, we will have to take the output of the auto.arima,
#and do the inverse operations to the data which are undifferencing and
#the inverse of log10.
#So, we left the job for the auto.arima function to do the differencing and undifferencing.
#The auto.arima function will always do the necessary (differencing/detrending) first
#to make the time series stationary, in order to be able to fit the model, and then will do the
#inverse operation back again (undifferencing/adding the trend) before generating the
#output of the forecast values.
#And we were left only with the job of inverting the logarithm which is an easy task.
#(13) library TSPred
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2018), range.percent = 0.2)
#(14) Forecast for a longer range.
plotarimapred(log10(data), ARIMAfit, xlim=c(2004,2022), range.percent = 0.2)
?auto.arima
#install.packages("wordcloud")
library('tm')
library("wordcloud")
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 8 - Text Analytics")
##########################
#req1
dfm <- read.csv("movie_reviews.csv")
##########################
#req2
dfm
dim(dfm)
##########################
#req3
pcorpus <- Corpus(VectorSource(dfm$text))
##########################
#req4
pcorpus <- tm_map(pcorpus, tolower)
##########################
#req5
#getTransformations()
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removeNumbers)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removePunctuation)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, stripWhitespace)
inspect(pcorpus[940])
##########################
#req6
pcorpus <- tm_map(pcorpus,removeWords, stopwords("english"))
inspect(pcorpus[940])
##########################
#req7
pdtm <- DocumentTermMatrix(pcorpus)
##########################
#req8
inspect(pdtm)
#Sparsity : 100% #??modify
#Non-/sparse entries: 689184/3006202016
#Sparsity = 300620201600/(3006202016 + 689184)
dim(pdtm)
##########################
#req9
pdtm <- removeSparseTerms(pdtm, 0.999)
#ignoring terms that have a document frequency lower than a given threshold
#doc freq = #doc containing the word/total #docs
str(inspect(pdtm))
#########################
#req10
pfreq <- findFreqTerms(pdtm, 65)
pfreq
#########################
#req11
movie_assoc <- findAssocs(pdtm, "titanic", 0.05)
movie_assoc
live_assoc <- findAssocs(pdtm, "marvel", 0.05)
live_assoc
#install.packages("wordcloud")
library('tm')
library("wordcloud")
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 8 - Text Analytics")
##########################
#req1
dfm <- read.csv("movie_reviews.csv")
##########################
#req2
dfm
dim(dfm)
##########################
#req3
pcorpus <- Corpus(VectorSource(dfm$text))
##########################
#req4
pcorpus <- tm_map(pcorpus, tolower)
##########################
#req5
#getTransformations()
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removeNumbers)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removePunctuation)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, stripWhitespace)
inspect(pcorpus[940])
##########################
#req6
pcorpus <- tm_map(pcorpus,removeWords, stopwords("english"))
inspect(pcorpus[940])
##########################
#req7
pdtm <- DocumentTermMatrix(pcorpus)
##########################
#req8
inspect(pdtm)
#Sparsity : 100% #??modify
#Non-/sparse entries: 689184/3006202016
#Sparsity = 300620201600/(3006202016 + 689184)
dim(pdtm)
##########################
#req9
pdtm <- removeSparseTerms(pdtm, 0.9999)
#ignoring terms that have a document frequency lower than a given threshold
#doc freq = #doc containing the word/total #docs
str(inspect(pdtm))
#########################
#req10
pfreq <- findFreqTerms(pdtm, 65)
pfreq
#########################
#req11
movie_assoc <- findAssocs(pdtm, "titanic", 0.05)
movie_assoc
live_assoc <- findAssocs(pdtm, "marvel", 0.05)
live_assoc
req12
pdtm <- removeSparseTerms(pdtm, 0.999)
#########################
#req13
pdtm2 <- as.matrix(pdtm)
pdtm2
pfrequency <- colSums(pdtm2)
pfrequency
pfrequency <- sort(pfrequency, decreasing = TRUE)
pfrequency
#########################
#req14
pfrequency[1:5]
#########################
#req15
pwords <- names(pfrequency[1:100])
wordcloud(pwords, pfrequency[1:100],colors = c("chartreuse", "cornflowerblue", "darkorange"))
#install.packages("wordcloud")
library('tm')
library("wordcloud")
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 8 - Text Analytics")
##########################
#req1
dfm <- read.csv("movie_reviews.csv")
##########################
#req2
dfm
dim(dfm)
##########################
#req3
pcorpus <- Corpus(VectorSource(dfm$text))
##########################
#req4
pcorpus <- tm_map(pcorpus, tolower)
##########################
#req5
#getTransformations()
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removeNumbers)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removePunctuation)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, stripWhitespace)
inspect(pcorpus[940])
##########################
#req6
pcorpus <- tm_map(pcorpus,removeWords, stopwords("english"))
inspect(pcorpus[940])
##########################
#req7
pdtm <- DocumentTermMatrix(pcorpus)
##########################
#req8
inspect(pdtm)
#Sparsity : 100% #??modify
#Non-/sparse entries: 689184/3006202016
#Sparsity = 300620201600/(3006202016 + 689184)
dim(pdtm)
##########################
#req9
pdtm <- removeSparseTerms(pdtm, 0.9999)
#ignoring terms that have a document frequency lower than a given threshold
#doc freq = #doc containing the word/total #docs
str(inspect(pdtm))
#########################
#req10
pfreq <- findFreqTerms(pdtm, 65)
pfreq
#########################
#req11
movie_assoc <- findAssocs(pdtm, "titanic", 0.05)
movie_assoc
live_assoc <- findAssocs(pdtm, "marvel", 0.05)
live_assoc
#######################
#req12
pdtm <- removeSparseTerms(pdtm, 0.999)
#########################
#req13
pdtm2 <- as.matrix(pdtm)
pdtm2
pfrequency <- colSums(pdtm2)
pfrequency
pfrequency <- sort(pfrequency, decreasing = TRUE)
pfrequency
#########################
#req14
pfrequency[1:5]
#########################
#req15
pwords <- names(pfrequency[1:100])
wordcloud(pwords, pfrequency[1:100],colors = c("chartreuse", "cornflowerblue", "darkorange"))
#install.packages("wordcloud")
library('tm')
library("wordcloud")
#clean environment
rm(list=ls())
#set working directory
setwd("E:\\college\\fourth_year\\4B\\BigData\\labs\\Lab 8 - Text Analytics")
##########################
#req1
dfm <- read.csv("movie_reviews.csv")
##########################
#req2
dfm
dim(dfm)
##########################
#req3
pcorpus <- Corpus(VectorSource(dfm$text))
##########################
#req4
pcorpus <- tm_map(pcorpus, tolower)
##########################
#req5
#getTransformations()
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removeNumbers)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, removePunctuation)
inspect(pcorpus[940])
pcorpus <- tm_map(pcorpus, stripWhitespace)
inspect(pcorpus[940])
##########################
#req6
pcorpus <- tm_map(pcorpus,removeWords, stopwords("english"))
inspect(pcorpus[940])
##########################
#req7
pdtm <- DocumentTermMatrix(pcorpus)
##########################
#req8
inspect(pdtm)
#Sparsity : 100% #??modify
#Non-/sparse entries: 689184/3006202016
#Sparsity = 300620201600/(3006202016 + 689184)
dim(pdtm)
##########################
#req9
pdtm <- removeSparseTerms(pdtm, 0.9999)
#ignoring terms that have a document frequency lower than a given threshold
#doc freq = #doc containing the word/total #docs
str(inspect(pdtm))
#########################
#req10
pfreq <- findFreqTerms(pdtm, 65)
pfreq
#########################
#req11
movie_assoc <- findAssocs(pdtm, "titanic", 0.05)
movie_assoc
live_assoc <- findAssocs(pdtm, "marvel", 0.05)
live_assoc
#######################
#req12
pdtm <- removeSparseTerms(pdtm, 0.999)
#########################
#req13
pdtm2 <- as.matrix(pdtm)
pdtm2
pfrequency <- colSums(pdtm2)
pfrequency
pfrequency <- sort(pfrequency, decreasing = TRUE)
pfrequency
#########################
#req14
pfrequency[1:5]
#########################
#req15
pwords <- names(pfrequency[1:100])
wordcloud(pwords, pfrequency[1:100],colors = c("chartreuse", "cornflowerblue", "darkorange"))
